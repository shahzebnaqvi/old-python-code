{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y3n1rKivjiw0"
   },
   "source": [
    "---\n",
    "[![Linkedin](https://content.linkedin.com/content/dam/brand/site/img/logo/logo-r.png) Join group for discussion on Machine Learning](https://www.linkedin.com/groups/13570295) \n",
    "\n",
    "---\n",
    "\n",
    "# License\n",
    "Copyright Â© 2018, Rohit Sharma. Paripath Inc.\n",
    "\n",
    "This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or(at your option) any later version.\n",
    "\n",
    "This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the [GNU General Public License](www.gnu.org/licenses/gpl-3.0.html) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "toc",
    "id": "WOwjsxUtjprT"
   },
   "source": [
    ">[License](#scrollTo=Y3n1rKivjiw0)\n",
    "\n",
    ">[10 Class Classification of Cells Using Neural Network](#scrollTo=-cI7TvCTiDS9)\n",
    "\n",
    ">>[Dataset](#scrollTo=7zTtLHoJiDTI)\n",
    "\n",
    ">[Load cells](#scrollTo=jpCXhhdxiDTL)\n",
    "\n",
    ">[Randomize Dataset](#scrollTo=2RuDlHBuiDTW)\n",
    "\n",
    ">[Reformat Dataset](#scrollTo=Zya2SABiiDTb)\n",
    "\n",
    ">[Normalize Dataset](#scrollTo=nPbbsIzviDTg)\n",
    "\n",
    ">[Divide dataset](#scrollTo=2TEdkp4xiDTv)\n",
    "\n",
    ">>[Declare Graph Variables](#scrollTo=XNug9X4TiDT5)\n",
    "\n",
    ">>[Compute Graph](#scrollTo=Tz2s2QP9iDT-)\n",
    "\n",
    ">>[Parameters and Function](#scrollTo=Qs5W2bZJiDUJ)\n",
    "\n",
    ">>[Training the Model](#scrollTo=cBgE5fQNiDUP)\n",
    "\n",
    ">[Cost and Accuracy](#scrollTo=FGFZl0M2iDUX)\n",
    "\n",
    ">[Confusion Matrix](#scrollTo=9I_oaPnhiDUe)\n",
    "\n",
    ">[Conclusion](#scrollTo=VvmAUHfpiDUk)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-cI7TvCTiDS9"
   },
   "source": [
    "# 10 Class Classification of Cells Using Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "IErJval4iDS_",
    "outputId": "e1785834-6526-40ed-871a-b33c08bbf70c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version     : 3.9\n",
      "Tensorflow version : 2.9.1\n",
      "numpy version      : 1.20.3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "import urllib.request; # for reading the data\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "print('Python version     :', '{0[0]}.{0[1]}'.format(sys.version_info))\n",
    "print('Tensorflow version :', tf.__version__)\n",
    "print('numpy version      :', np.__version__)\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7zTtLHoJiDTI"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "Our dataset consists of total of over 4,200 cells with a total of 10 categories;  \n",
    "Each cell in the dataset is represented as a 2D matrix of transistor devices and their parameters given below:\n",
    "\n",
    "\n",
    "| Device | param1 | param2 | param3 | param4 | param5 | ...... | ...... | param13 |\n",
    "|--------|--------|--------|--------|--------|--------|--------|--------|---------|\n",
    "|   10   |    6   |    6   |    3   |    3   |    5   |        |        |    2    |\n",
    "|   10   |    8   |    3   |    2   |    4   |    4   |    2   |    3   |    6    |\n",
    "|   20   |    7   |    3   |    2   |    4   |    4   |    2   |    3   |    2    |\n",
    "|  ...   |  ...   |  ...   |  ...   |  ...   |  ...   |  ...   |  ...   |  ...    |\n",
    "|  ...   |  ...   |  ...   |  ...   |  ...   |  ...   |  ...   |  ...   |  ...    |\n",
    "|   20   |    7   |    3   |    2   |    4   |    5   |    2   |    3   |    2    |\n",
    "\n",
    "\n",
    "Entire dataset consists of 4,200 such tables is shown in the next picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gVPPGDbliDTJ"
   },
   "source": [
    "\n",
    "![Cell Dataset As Stack Of Tables](https://github.com/srohit0/mida/blob/master/data/CellDatasetAsStackOfTables.jpg?raw=true =580x560)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jpCXhhdxiDTL"
   },
   "source": [
    "# Load cells\n",
    "\n",
    "1. Set parameters of the cells (like max devices and feature per device etc).\n",
    "2. Read cells off the the internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H0B1EMpKiDTL"
   },
   "outputs": [],
   "source": [
    "# max_devices_per_cell is also max number of rows in circuit files in the training set\n",
    "max_devices_per_cell = 369; \n",
    "# feature_per_device is different from max number of cols (14) in circuit files in training set\n",
    "feature_per_device   = 5; \n",
    "feature_indices = (0,2,5,8,11)\n",
    "\n",
    "def load_one_cell(cell_file):\n",
    "    cell_cols = 14\n",
    "    cell_data = np.zeros(shape=(max_devices_per_cell, cell_cols), dtype=np.float32)\n",
    "    #print(cell_file)\n",
    "    decoded_cell  = urllib.request.urlopen(cell_file)\n",
    "    cell_mat  = decoded_cell.read().decode('utf-8')\n",
    "    cell_mat  = np.fromstring(cell_mat, dtype=int, sep=' ').reshape(-1, cell_cols)\n",
    "    nrows     = cell_mat.shape[0]\n",
    "    ncols     = cell_mat.shape[1]\n",
    "    cell_data[:nrows, :ncols] = cell_mat\n",
    "    cell_data = cell_data[:, feature_indices]\n",
    "    return cell_data\n",
    "\n",
    "#load_one_cell('https://raw.githubusercontent.com/srohit0/mida/master/data/cells/FDN/FDNQ_1.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "colab_type": "code",
    "id": "p8YyQAcPiDTQ",
    "outputId": "5ea6ac1f-a687-4567-b765-93a1fad39efc"
   },
   "outputs": [],
   "source": [
    "# return list of files with ext in a dir over http\n",
    "def httpDir(url, ext=''):\n",
    "#     from urllib.parse import urlparse\n",
    "#     parsed_uri = urlparse(url)\n",
    "#     base_url   = '{uri.scheme}://{uri.netloc}/'.format(uri=parsed_uri)\n",
    "    base_url = 'https://raw.githubusercontent.com'\n",
    "\n",
    "    page = requests.get(url).text\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    return [base_url + node.get('href').replace('blob/', '') for node in soup.find_all('a') if node.get('href').endswith(ext)]\n",
    " \n",
    "def load_cells(folder):\n",
    "  #Load the data for a single label.\n",
    "  cell_files = httpDir(folder, 'mat')\n",
    "  #print(cell_files)\n",
    "  dataset = np.zeros(shape=(len(cell_files), max_devices_per_cell, feature_per_device),\n",
    "                         dtype=np.float32)\n",
    "  cell_index = 0\n",
    "  for cell_file in cell_files:\n",
    "    cell = os.path.basename(cell_file)\n",
    "    try:\n",
    "      cell_data = load_one_cell(cell_file)\n",
    "      #print (cell, 'cell data', type(cell_data))\n",
    "\n",
    "      cell_rows = cell_data.shape[0]\n",
    "      cell_cols = cell_data.shape[1]\n",
    "      dataset[cell_index, :cell_rows, :cell_cols] = cell_data\n",
    "\n",
    "      cell_index = cell_index + 1\n",
    "    except IOError as e:\n",
    "      print('Could not read:', cell_file, ':', e, '- it\\'s ok, skipping.')\n",
    "    \n",
    "  dataset = dataset[0:cell_index, :, :]\n",
    "\n",
    "  print(os.path.basename(folder), ': shape=', dataset.shape, ', Mean=', np.mean(dataset), ', sigma=', np.std(dataset))\n",
    "  return dataset\n",
    "\n",
    "label_list = ['FDN', 'LD', 'ADDH', 'MAJ', 'AOA', 'ADDF', 'CKG', 'FDP', 'INV', 'EN']\n",
    "n_labels   = len(label_list); # num_classes\n",
    "for label_index in range(n_labels):\n",
    "    cell_dataset = load_cells('https://github.com/srohit0/mida/raw/master/data/cells/' + label_list[label_index])\n",
    "    #print (type(cell_dataset))\n",
    "    cell_labels = np.zeros((cell_dataset.shape[0], n_labels))\n",
    "    cell_labels[:, label_index] = 1\n",
    "    if label_index:\n",
    "        dataset = np.append(dataset, cell_dataset, axis=0)\n",
    "        labels  = np.append(labels,  cell_labels,  axis=0)\n",
    "    else:\n",
    "        dataset = cell_dataset\n",
    "        labels  = cell_labels\n",
    "\n",
    "print('dataset: shape=', dataset.shape, ', Mean=', np.mean(dataset), ', sigma=', np.std(dataset))\n",
    "print('labels : shape=', labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2RuDlHBuiDTW"
   },
   "source": [
    "# Randomize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "dDBstqPqiDTW",
    "outputId": "8bc24388-c6af-4f91-efd8-f299741c1759"
   },
   "outputs": [],
   "source": [
    "# randomize dataset and labels together to keep'em in sync.\n",
    "def randomize(dataset, labels):\n",
    "  permutation = np.random.permutation(labels.shape[0])\n",
    "  shuffled_dataset = dataset[permutation,:,:]\n",
    "  shuffled_labels = labels[permutation]\n",
    "  return shuffled_dataset, shuffled_labels\n",
    "\n",
    "# randomize dataset\n",
    "dataset, labels = randomize(dataset, labels)\n",
    "\n",
    "print('Full dataset: no. of cells=', dataset.shape, 'Min=', np.min(dataset), 'Max=',np.max(dataset), \n",
    "      'Mean=', np.mean(dataset), 'Sigma=', np.std(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zya2SABiiDTb"
   },
   "source": [
    "# Reformat Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "T1SU117piDTb",
    "outputId": "2241865e-1d11-41ac-ea4d-291432e3a815"
   },
   "outputs": [],
   "source": [
    "def reformat(dataset):\n",
    "    dataset = dataset.reshape((-1, max_devices_per_cell*feature_per_device))\n",
    "    return dataset\n",
    "\n",
    "dataset = reformat(dataset)\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nPbbsIzviDTg"
   },
   "source": [
    "# Normalize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "nxKrbpUMiDTh",
    "outputId": "900f205a-1987-42af-fc94-bdc4314f563e"
   },
   "outputs": [],
   "source": [
    "# normalize the dataset between 0 and 1.\n",
    "data_mean = 0;\n",
    "data_std  = (np.max(dataset)-np.min(dataset));\n",
    "\n",
    "def normalize(d, mean, std):\n",
    "    return (d - mean) / std\n",
    "\n",
    "def denormalize(d, mean, std):\n",
    "    return (d * std) + mean\n",
    "\n",
    "dataset = normalize(dataset, data_mean, data_std)\n",
    "\n",
    "print('Min=', np.min(dataset), 'Max=',np.max(dataset), 'Mean=',np.mean(dataset), 'sigma=', np.std(dataset))\n",
    "print('Dataset size', dataset.shape)\n",
    "#print('dataset feature min vector = ', np.min(dataset, axis=0))\n",
    "#print('dataset feature max vector = ', np.max(dataset, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2TEdkp4xiDTv"
   },
   "source": [
    "# Divide dataset\n",
    "into training dataset and validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "X2hr6Ce9iDTw",
    "outputId": "0a2647bc-20f4-4691-ac1e-d260fb06a3cb"
   },
   "outputs": [],
   "source": [
    "# training dataset percentage is 80%\n",
    "def divide_dataset(dataset, labels):\n",
    "    train_pct = 0.8; \n",
    "    # divide dataset into training and validation set\n",
    "    train_index = int(dataset.shape[0]*train_pct)\n",
    "    t_X = dataset[:train_index, :]\n",
    "    t_Y = labels[:train_index,:]\n",
    "    v_X = dataset[train_index:,:]\n",
    "    v_Y = labels[train_index:,:]\n",
    "    \n",
    "    return t_X, t_Y, v_X, v_Y\n",
    "\n",
    "train_X, train_Y, valid_X, valid_Y = divide_dataset(dataset, labels)\n",
    "\n",
    "print(train_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XNug9X4TiDT5"
   },
   "source": [
    "## Declare Graph Variables\n",
    "\n",
    "Place to declare place holders and variables for compute graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "itF2dWiniDT6"
   },
   "outputs": [],
   "source": [
    "SIGMA      = np.sqrt(2.0/train_X.shape[0])\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 1e-2\n",
    "l2_loss_beta  = 1e-5\n",
    "training_epochs = 1000\n",
    "display_step = 50\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of neurons\n",
    "n_hidden_2 = 32   # 2nd layer number of neurons\n",
    "n_features = train_X.shape[1]; # num_inputs\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "tf.disable_v2_behavior()\n",
    "# tf Graph input\n",
    "t_dataset = tf.placeholder(tf.float32, shape=(None, n_features), name='t_dataset');\n",
    "t_labels  = tf.placeholder(tf.float32, shape=(None, n_labels), name='t_labels');\n",
    "\n",
    "\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.truncated_normal(shape=[n_features, n_hidden_1], mean=3.0*SIGMA, \n",
    "                                          stddev=SIGMA, dtype=tf.float32), trainable=True),\n",
    "    'h2': tf.Variable(tf.truncated_normal(shape=[n_hidden_1, n_hidden_2], mean=3.0*SIGMA, \n",
    "                                           stddev=SIGMA, dtype=tf.float32), trainable=True),\n",
    "    'out': tf.Variable(tf.truncated_normal(shape=[n_hidden_2, n_labels], mean=3.0*SIGMA, \n",
    "                                           stddev=SIGMA, dtype=tf.float32), trainable=True)\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.constant(value=SIGMA, dtype=tf.float32, shape=[n_hidden_1]), trainable=True),\n",
    "    'b2': tf.Variable(tf.constant(value=SIGMA, dtype=tf.float32, shape=[n_hidden_2]), trainable=True),\n",
    "    'out': tf.Variable(tf.constant(value=SIGMA, dtype=tf.float32, shape=[n_labels]), trainable=True)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tz2s2QP9iDT-"
   },
   "source": [
    "## Compute Graph\n",
    "Construct a Neural Network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r3CDUn-QiDUC"
   },
   "outputs": [],
   "source": [
    "def neural_net(x):\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1a = tf.nn.relu6(layer_1); #activation\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_2 = tf.add(tf.matmul(layer_1a, weights['h2']), biases['b2'])\n",
    "    layer_2a = tf.nn.relu6(layer_2) #activation\n",
    "    # Output fully connected layer with a neuron for each class\n",
    "    out_layer = tf.matmul(layer_2a, weights['out']) + biases['out']\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qs5W2bZJiDUJ"
   },
   "source": [
    "## Parameters and Function\n",
    "Neural network parameters and functionss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P8nK-EmxiDUK"
   },
   "outputs": [],
   "source": [
    "# Construct model\n",
    "predictions = neural_net(t_dataset)\n",
    "\n",
    "t_labels_cls       = tf.argmax(t_labels, axis=1)\n",
    "predictions_cls    = tf.argmax(predictions, axis=1)\n",
    "correct_prediction = tf.equal(predictions_cls, t_labels_cls)\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# \n",
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=predictions,\n",
    "                                                               labels=t_labels_cls)\n",
    "\n",
    "# L2 loss\n",
    "regularizers = tf.nn.l2_loss(weights['h1']) + tf.nn.l2_loss(weights['h2']) + tf.nn.l2_loss(weights['out'])\n",
    "\n",
    "# Cost\n",
    "cost = tf.reduce_mean(cross_entropy + l2_loss_beta * regularizers)\n",
    "\n",
    "# Optimization algorithm\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cBgE5fQNiDUP"
   },
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "colab_type": "code",
    "id": "IkxQUbjQiDUR",
    "outputId": "409593dc-779f-496e-a93a-45b2949091bf"
   },
   "outputs": [],
   "source": [
    "# Start training\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "t_start = datetime.now()\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "\n",
    "#     sess.run(optimizer, feed_dict={t_dataset:train_X, t_labels:train_Y})\n",
    "    sess.run(optimizer, feed_dict={train_X, train_Y})\n",
    "\n",
    "    #Display logs per epoch step\n",
    "    if (epoch+1) % display_step == 0:\n",
    "\n",
    "        t_cost, t_acc = sess.run([cost, accuracy], feed_dict={t_dataset:train_X, t_labels:train_Y})\n",
    "        v_cost, v_acc = sess.run([cost, accuracy], feed_dict={t_dataset:valid_X, t_labels:valid_Y})\n",
    "        print (\"[\", \"{:4d}\".format((datetime.now()-t_start).seconds), \n",
    "               \"sec ] Epoch:\", '%04d' % (epoch+1), \n",
    "               \"cost=\", \"{:.5f}\".format(t_cost),\n",
    "               \"train acc=\", \"{:.5f}\".format(t_acc),\n",
    "               \"valid acc=\", \"{:.3f}\".format(v_acc))\n",
    "\n",
    "print( \"\\nOptimization Finished!\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FGFZl0M2iDUX"
   },
   "source": [
    "# Cost and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "OwTR-UIbiDUY",
    "outputId": "6a07b168-d5d0-499c-881e-aee24953e1da"
   },
   "outputs": [],
   "source": [
    "# Find cost on training dataset.\n",
    "t_cost, t_acc = sess.run([cost, accuracy], feed_dict={t_dataset:train_X, t_labels:train_Y})\n",
    "v_cost, v_acc = sess.run([cost, accuracy], feed_dict={t_dataset:valid_X, t_labels:valid_Y})\n",
    "\n",
    "print (\"Training   cost    =\", t_cost)\n",
    "print (\"Validation cost    =\", v_cost, '\\n')\n",
    "print(\"Training   Accuracy =\", t_acc)\n",
    "print(\"Validation Accuracy =\", v_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9I_oaPnhiDUe"
   },
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 593
    },
    "colab_type": "code",
    "id": "LDPhhOKRiDUe",
    "outputId": "1ea04149-c028-4d14-87e8-a0ee09146030"
   },
   "outputs": [],
   "source": [
    "pred = sess.run(predictions, feed_dict={t_dataset:dataset, t_labels:labels})\n",
    "cm = tf.confusion_matrix(labels=np.argmax(labels, axis=1), predictions=np.argmax(pred, axis=1), num_classes=n_labels)\n",
    "cm_ary = cm.eval(session=sess)\n",
    "\n",
    "plt.figure(figsize = (n_labels,n_labels))\n",
    "sn.heatmap(cm_ary,   xticklabels=label_list, yticklabels=label_list,\n",
    "           cmap=\"Blues\", annot=True, fmt='d', annot_kws={\"size\": 12})# font size\n",
    "sn.set(font_scale=1.0)#for label size\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VvmAUHfpiDUk"
   },
   "source": [
    "# Conclusion\n",
    "Neural networks gave us reasonable accuracy of over 90% in both training and validation sets.\n",
    "\n",
    "In the confusion matrix, it is easy to see that EN (ExNor function) and MAJ (majority function) have large numbers of false predicions. \n",
    "\n",
    "We also know that these two categories of cells have similar structural implementation. It is speculated that neural network **`magically`** figures this out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_5r5C9Q_iDUm"
   },
   "outputs": [],
   "source": [
    "#sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a-t1M_WdjyHN"
   },
   "source": [
    "# Credit:\n",
    "\n",
    "[Machine Intelligence in Design Automation](amzn.to/2paZ53b)\n",
    "\n",
    "---\n",
    "\n",
    "[![Machine Intelligence in Design Automation](https://qph.ec.quoracdn.net/main-qimg-be4b2dd87b0ef610f4fda06b89904bd5)](http://amzn.to/2paZ53b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "10 class Classification Of Cells Using FFNN-RELU.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
